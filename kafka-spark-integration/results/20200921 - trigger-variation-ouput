root@81c68c0b4a46:/home/workspace# spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.4 trigger_variation.py
Ivy Default Cache set to: /root/.ivy2/cache
The jars for the packages stored in: /root/.ivy2/jars
:: loading settings :: url = jar:file:/opt/spark-2.3.4-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
org.apache.spark#spark-sql-kafka-0-10_2.11 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-74700b58-af8a-47a3-bc11-596c754d3273;1.0
        confs: [default]
        found org.apache.spark#spark-sql-kafka-0-10_2.11;2.3.4 in central
        found org.apache.kafka#kafka-clients;0.10.0.1 in central
        found net.jpountz.lz4#lz4;1.3.0 in central
        found org.xerial.snappy#snappy-java;1.1.2.6 in central
        found org.slf4j#slf4j-api;1.7.16 in central
        found org.spark-project.spark#unused;1.0.0 in central
:: resolution report :: resolve 732ms :: artifacts dl 14ms
        :: modules in use:
        net.jpountz.lz4#lz4;1.3.0 from central in [default]
        org.apache.kafka#kafka-clients;0.10.0.1 from central in [default]
        org.apache.spark#spark-sql-kafka-0-10_2.11;2.3.4 from central in [default]
        org.slf4j#slf4j-api;1.7.16 from central in [default]
        org.spark-project.spark#unused;1.0.0 from central in [default]
        org.xerial.snappy#snappy-java;1.1.2.6 from central in [default]
        ---------------------------------------------------------------------
        |                  |            modules            ||   artifacts   |
        |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
        ---------------------------------------------------------------------
        |      default     |   6   |   0   |   0   |   0   ||   6   |   0   |
        ---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-74700b58-af8a-47a3-bc11-596c754d3273
        confs: [default]
        0 artifacts copied, 6 already retrieved (0kB/13ms)
2020-09-21 01:18:53 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-09-21 01:18:55 INFO  SparkContext:54 - Running Spark version 2.3.4
2020-09-21 01:18:55 INFO  SparkContext:54 - Submitted application: StructuredStreamingSetup
2020-09-21 01:18:55 INFO  SecurityManager:54 - Changing view acls to: root
2020-09-21 01:18:55 INFO  SecurityManager:54 - Changing modify acls to: root
2020-09-21 01:18:55 INFO  SecurityManager:54 - Changing view acls groups to: 
2020-09-21 01:18:55 INFO  SecurityManager:54 - Changing modify acls groups to: 
2020-09-21 01:18:55 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
2020-09-21 01:18:55 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 33251.
2020-09-21 01:18:55 INFO  SparkEnv:54 - Registering MapOutputTracker
2020-09-21 01:18:55 INFO  SparkEnv:54 - Registering BlockManagerMaster
2020-09-21 01:18:55 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-09-21 01:18:55 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up
2020-09-21 01:18:55 INFO  DiskBlockManager:54 - Created local directory at /tmp/blockmgr-2cdd52fe-7aef-4ab1-8495-9cff2e82f9b3
2020-09-21 01:18:55 INFO  MemoryStore:54 - MemoryStore started with capacity 366.3 MB
2020-09-21 01:18:55 INFO  SparkEnv:54 - Registering OutputCommitCoordinator
2020-09-21 01:18:55 INFO  log:192 - Logging initialized @4416ms
2020-09-21 01:18:56 INFO  Server:351 - jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2020-09-21 01:18:56 INFO  Server:419 - Started @4598ms
2020-09-21 01:18:56 INFO  AbstractConnector:278 - Started ServerConnector@e1b68f8{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-09-21 01:18:56 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4040.
2020-09-21 01:18:56 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@657b4e62{/jobs,null,AVAILABLE,@Spark}
2020-09-21 01:18:56 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@52872966{/jobs/json,null,AVAILABLE,@Spark}
2020-09-21 01:18:56 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3470206e{/jobs/job,null,AVAILABLE,@Spark}
2020-09-21 01:18:56 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@dded089{/jobs/job/json,null,AVAILABLE,@Spark}
2020-09-21 01:18:56 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6679144d{/stages,null,AVAILABLE,@Spark}
2020-09-21 01:18:56 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@533f2bbc{/stages/json,null,AVAILABLE,@Spark}
2020-09-21 01:18:56 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@877a303{/stages/stage,null,AVAILABLE,@Spark}
2020-09-21 01:18:56 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@77eb7b84{/stages/stage/json,null,AVAILABLE,@Spark}
2020-09-21 01:18:56 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@42b1b1c1{/stages/pool,null,AVAILABLE,@Spark}
2020-09-21 01:18:56 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@227ef1b{/stages/pool/json,null,AVAILABLE,@Spark}
2020-09-21 01:18:56 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@410de7e9{/storage,null,AVAILABLE,@Spark}
2020-09-21 01:18:56 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6d968317{/storage/json,null,AVAILABLE,@Spark}
2020-09-21 01:18:56 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@22e618dd{/storage/rdd,null,AVAILABLE,@Spark}
2020-09-21 01:18:56 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1f0464ad{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-09-21 01:18:56 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3118fa0b{/environment,null,AVAILABLE,@Spark}
2020-09-21 01:18:56 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@68cf4904{/environment/json,null,AVAILABLE,@Spark}
2020-09-21 01:18:56 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7a7d9430{/executors,null,AVAILABLE,@Spark}
2020-09-21 01:18:56 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4660a6de{/executors/json,null,AVAILABLE,@Spark}
2020-09-21 01:18:56 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@305ed4bb{/executors/threadDump,null,AVAILABLE,@Spark}
2020-09-21 01:18:56 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@22cb3cdd{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-09-21 01:18:56 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3401f0a0{/static,null,AVAILABLE,@Spark}
2020-09-21 01:18:56 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3a67d0a7{/,null,AVAILABLE,@Spark}
2020-09-21 01:18:56 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@711af4a9{/api,null,AVAILABLE,@Spark}
2020-09-21 01:18:56 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2957f320{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-09-21 01:18:56 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@28a64a1b{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-09-21 01:18:56 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://81c68c0b4a46:4040
2020-09-21 01:18:56 INFO  SparkContext:54 - Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.11-2.3.4.jar at spark://81c68c0b4a46:33251/jars/org.apache.spark_spark-sql-kafka-0-10_2.11-2.3.4.jar with timestamp 1600651136432
2020-09-21 01:18:56 INFO  SparkContext:54 - Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.10.0.1.jar at spark://81c68c0b4a46:33251/jars/org.apache.kafka_kafka-clients-0.10.0.1.jar with timestamp 1600651136439
2020-09-21 01:18:56 INFO  SparkContext:54 - Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://81c68c0b4a46:33251/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1600651136450
2020-09-21 01:18:56 INFO  SparkContext:54 - Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.3.0.jar at spark://81c68c0b4a46:33251/jars/net.jpountz.lz4_lz4-1.3.0.jar with timestamp 1600651136453
2020-09-21 01:18:56 INFO  SparkContext:54 - Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.2.6.jar at spark://81c68c0b4a46:33251/jars/org.xerial.snappy_snappy-java-1.1.2.6.jar with timestamp 1600651136454
2020-09-21 01:18:56 INFO  SparkContext:54 - Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://81c68c0b4a46:33251/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1600651136459
2020-09-21 01:18:56 INFO  SparkContext:54 - Added file file:/home/workspace/trigger_variation.py at file:/home/workspace/trigger_variation.py with timestamp 1600651136503
2020-09-21 01:18:56 INFO  Utils:54 - Copying /home/workspace/trigger_variation.py to /tmp/spark-a12725d7-416b-4f31-bbb7-41d9429580d5/userFiles-c548600c-84c8-4fe2-acf7-54ea84b37ca9/trigger_variation.py
2020-09-21 01:18:56 INFO  SparkContext:54 - Added file file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.11-2.3.4.jar at file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.11-2.3.4.jar with timestamp 1600651136523
2020-09-21 01:18:56 INFO  Utils:54 - Copying /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.11-2.3.4.jar to /tmp/spark-a12725d7-416b-4f31-bbb7-41d9429580d5/userFiles-c548600c-84c8-4fe2-acf7-54ea84b37ca9/org.apache.spark_spark-sql-kafka-0-10_2.11-2.3.4.jar
2020-09-21 01:18:56 INFO  SparkContext:54 - Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.10.0.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.10.0.1.jar with timestamp 1600651136538
2020-09-21 01:18:56 INFO  Utils:54 - Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.10.0.1.jar to /tmp/spark-a12725d7-416b-4f31-bbb7-41d9429580d5/userFiles-c548600c-84c8-4fe2-acf7-54ea84b37ca9/org.apache.kafka_kafka-clients-0.10.0.1.jar
2020-09-21 01:18:56 INFO  SparkContext:54 - Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1600651136554
2020-09-21 01:18:56 INFO  Utils:54 - Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-a12725d7-416b-4f31-bbb7-41d9429580d5/userFiles-c548600c-84c8-4fe2-acf7-54ea84b37ca9/org.spark-project.spark_unused-1.0.0.jar
2020-09-21 01:18:56 INFO  SparkContext:54 - Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.3.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.3.0.jar with timestamp 1600651136567
2020-09-21 01:18:56 INFO  Utils:54 - Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.3.0.jar to /tmp/spark-a12725d7-416b-4f31-bbb7-41d9429580d5/userFiles-c548600c-84c8-4fe2-acf7-54ea84b37ca9/net.jpountz.lz4_lz4-1.3.0.jar
2020-09-21 01:18:56 INFO  SparkContext:54 - Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.2.6.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.2.6.jar with timestamp 1600651136579
2020-09-21 01:18:56 INFO  Utils:54 - Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.2.6.jar to /tmp/spark-a12725d7-416b-4f31-bbb7-41d9429580d5/userFiles-c548600c-84c8-4fe2-acf7-54ea84b37ca9/org.xerial.snappy_snappy-java-1.1.2.6.jar
2020-09-21 01:18:56 INFO  SparkContext:54 - Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1600651136594
2020-09-21 01:18:56 INFO  Utils:54 - Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-a12725d7-416b-4f31-bbb7-41d9429580d5/userFiles-c548600c-84c8-4fe2-acf7-54ea84b37ca9/org.slf4j_slf4j-api-1.7.16.jar
2020-09-21 01:18:56 INFO  Executor:54 - Starting executor ID driver on host localhost
2020-09-21 01:18:56 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40671.
2020-09-21 01:18:56 INFO  NettyBlockTransferService:54 - Server created on 81c68c0b4a46:40671
2020-09-21 01:18:56 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-09-21 01:18:56 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, 81c68c0b4a46, 40671, None)
2020-09-21 01:18:56 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 81c68c0b4a46:40671 with 366.3 MB RAM, BlockManagerId(driver, 81c68c0b4a46, 40671, None)
2020-09-21 01:18:56 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, 81c68c0b4a46, 40671, None)
2020-09-21 01:18:56 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, 81c68c0b4a46, 40671, None)
2020-09-21 01:18:57 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2e650772{/metrics/json,null,AVAILABLE,@Spark}
2020-09-21 01:18:57 INFO  SharedState:54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/workspace/spark-warehouse/').
2020-09-21 01:18:57 INFO  SharedState:54 - Warehouse path is 'file:/home/workspace/spark-warehouse/'.
2020-09-21 01:18:57 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@55664c21{/SQL,null,AVAILABLE,@Spark}
2020-09-21 01:18:57 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@37649b75{/SQL/json,null,AVAILABLE,@Spark}
2020-09-21 01:18:57 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@59b99cca{/SQL/execution,null,AVAILABLE,@Spark}
2020-09-21 01:18:57 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5aa6e435{/SQL/execution/json,null,AVAILABLE,@Spark}
2020-09-21 01:18:57 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7e08e7af{/static/sql,null,AVAILABLE,@Spark}
2020-09-21 01:18:57 INFO  StateStoreCoordinatorRef:54 - Registered StateStoreCoordinator endpoint
2020-09-21 01:18:57 INFO  ConsumerConfig:178 - ConsumerConfig values: 
        metric.reporters = []
        metadata.max.age.ms = 300000
        partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
        reconnect.backoff.ms = 50
        sasl.kerberos.ticket.renew.window.factor = 0.8
        max.partition.fetch.bytes = 1048576
        bootstrap.servers = [localhost:9092]
        ssl.keystore.type = JKS
        enable.auto.commit = false
        sasl.mechanism = GSSAPI
        interceptor.classes = null
        exclude.internal.topics = true
        ssl.truststore.password = null
        client.id = 
        ssl.endpoint.identification.algorithm = null
        max.poll.records = 1
        check.crcs = true
        request.timeout.ms = 40000
        heartbeat.interval.ms = 3000
        auto.commit.interval.ms = 5000
        receive.buffer.bytes = 65536
        ssl.truststore.type = JKS
        ssl.truststore.location = null
        ssl.keystore.password = null
        fetch.min.bytes = 1
        send.buffer.bytes = 131072
        value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
        group.id = spark-kafka-source-aad2ca7c-ebee-4da5-bc28-eb91f6c9a90a-1440725118-driver-0
        retry.backoff.ms = 100
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        ssl.trustmanager.algorithm = PKIX
        ssl.key.password = null
        fetch.max.wait.ms = 500
        sasl.kerberos.min.time.before.relogin = 60000
        connections.max.idle.ms = 540000
        session.timeout.ms = 30000
        metrics.num.samples = 2
        key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
        ssl.protocol = TLS
        ssl.provider = null
        ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
        ssl.keystore.location = null
        ssl.cipher.suites = null
        security.protocol = PLAINTEXT
        ssl.keymanager.algorithm = SunX509
        metrics.sample.window.ms = 30000
        auto.offset.reset = earliest

2020-09-21 01:18:58 INFO  ConsumerConfig:178 - ConsumerConfig values: 
        metric.reporters = []
        metadata.max.age.ms = 300000
        partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
        reconnect.backoff.ms = 50
        sasl.kerberos.ticket.renew.window.factor = 0.8
        max.partition.fetch.bytes = 1048576
        bootstrap.servers = [localhost:9092]
        ssl.keystore.type = JKS
        enable.auto.commit = false
        sasl.mechanism = GSSAPI
        interceptor.classes = null
        exclude.internal.topics = true
        ssl.truststore.password = null
        client.id = consumer-1
        ssl.endpoint.identification.algorithm = null
        max.poll.records = 1
        check.crcs = true
        request.timeout.ms = 40000
        heartbeat.interval.ms = 3000
        auto.commit.interval.ms = 5000
        receive.buffer.bytes = 65536
        ssl.truststore.type = JKS
        ssl.truststore.location = null
        ssl.keystore.password = null
        fetch.min.bytes = 1
        send.buffer.bytes = 131072
        value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
        group.id = spark-kafka-source-aad2ca7c-ebee-4da5-bc28-eb91f6c9a90a-1440725118-driver-0
        retry.backoff.ms = 100
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        ssl.trustmanager.algorithm = PKIX
        ssl.key.password = null
        fetch.max.wait.ms = 500
        sasl.kerberos.min.time.before.relogin = 60000
        connections.max.idle.ms = 540000
        session.timeout.ms = 30000
        metrics.num.samples = 2
        key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
        ssl.protocol = TLS
        ssl.provider = null
        ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
        ssl.keystore.location = null
        ssl.cipher.suites = null
        security.protocol = PLAINTEXT
        ssl.keymanager.algorithm = SunX509
        metrics.sample.window.ms = 30000
        auto.offset.reset = earliest

2020-09-21 01:18:58 INFO  AppInfoParser:83 - Kafka version : 0.10.0.1
2020-09-21 01:18:58 INFO  AppInfoParser:84 - Kafka commitId : a7a17cdec9eaa6c5
root
 |-- key: binary (nullable = true)
 |-- value: binary (nullable = true)
 |-- topic: string (nullable = true)
 |-- partition: integer (nullable = true)
 |-- offset: long (nullable = true)
 |-- timestamp: timestamp (nullable = true)
 |-- timestampType: integer (nullable = true)

Traceback (most recent call last):
  File "/opt/spark-2.3.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
  File "/opt/spark-2.3.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o41.count.
: org.apache.spark.sql.AnalysisException: Queries with streaming sources must be executed with writeStream.start();;
kafka
        at org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.org$apache$spark$sql$catalyst$analysis$UnsupportedOperationChecker$$throwError(UnsupportedOperationChecker.scala:374)
        at org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$$anonfun$checkForBatch$1.apply(UnsupportedOperationChecker.scala:37)
        at org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$$anonfun$checkForBatch$1.apply(UnsupportedOperationChecker.scala:35)
        at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)
        at scala.collection.immutable.List.foreach(List.scala:381)
        at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)
        at org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.checkForBatch(UnsupportedOperationChecker.scala:35)
        at org.apache.spark.sql.execution.QueryExecution.assertSupported(QueryExecution.scala:51)
        at org.apache.spark.sql.execution.QueryExecution.withCachedData$lzycompute(QueryExecution.scala:62)
        at org.apache.spark.sql.execution.QueryExecution.withCachedData(QueryExecution.scala:60)
        at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:66)
        at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:66)
        at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:72)
        at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:68)
        at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:77)
        at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:77)
        at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3260)
        at org.apache.spark.sql.Dataset.count(Dataset.scala:2780)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:748)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/workspace/trigger_variation.py", line 43, in <module>
    run_spark_job(spark)
  File "/home/workspace/trigger_variation.py", line 20, in run_spark_job
    agg_df = df.count()
  File "/opt/spark-2.3.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/dataframe.py", line 455, in count
  File "/opt/spark-2.3.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
  File "/opt/spark-2.3.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/utils.py", line 69, in deco
pyspark.sql.utils.AnalysisException: 'Queries with streaming sources must be executed with writeStream.start();;\nkafka'
2020-09-21 01:19:00 INFO  SparkContext:54 - Invoking stop() from shutdown hook
2020-09-21 01:19:00 INFO  AbstractConnector:318 - Stopped Spark@e1b68f8{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-09-21 01:19:00 INFO  SparkUI:54 - Stopped Spark web UI at http://81c68c0b4a46:4040
2020-09-21 01:19:00 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!
2020-09-21 01:19:00 INFO  MemoryStore:54 - MemoryStore cleared
2020-09-21 01:19:00 INFO  BlockManager:54 - BlockManager stopped
2020-09-21 01:19:00 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped
2020-09-21 01:19:00 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!
2020-09-21 01:19:00 INFO  SparkContext:54 - Successfully stopped SparkContext
2020-09-21 01:19:00 INFO  ShutdownHookManager:54 - Shutdown hook called
2020-09-21 01:19:00 INFO  ShutdownHookManager:54 - Deleting directory /tmp/temporaryReader-427d1dfc-85e4-4ebd-bdcd-c82f6da3230e
2020-09-21 01:19:00 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-a12725d7-416b-4f31-bbb7-41d9429580d5/pyspark-a52e11ce-1509-4836-9dd4-afcb8d676e2a
2020-09-21 01:19:00 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-d71305bb-61f0-40de-b3ba-51b2068ca545
2020-09-21 01:19:00 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-a12725d7-416b-4f31-bbb7-41d9429580d5
