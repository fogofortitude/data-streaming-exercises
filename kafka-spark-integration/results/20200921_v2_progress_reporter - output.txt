root@81c68c0b4a46:/home/workspace# spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.4 progress_reporter.py
Ivy Default Cache set to: /root/.ivy2/cache
The jars for the packages stored in: /root/.ivy2/jars
:: loading settings :: url = jar:file:/opt/spark-2.3.4-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
org.apache.spark#spark-sql-kafka-0-10_2.11 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-61ec7441-86a5-4a38-81b1-2153065117f4;1.0
        confs: [default]
        found org.apache.spark#spark-sql-kafka-0-10_2.11;2.3.4 in central
        found org.apache.kafka#kafka-clients;0.10.0.1 in central
        found net.jpountz.lz4#lz4;1.3.0 in central
        found org.xerial.snappy#snappy-java;1.1.2.6 in central
        found org.slf4j#slf4j-api;1.7.16 in central
        found org.spark-project.spark#unused;1.0.0 in central
:: resolution report :: resolve 694ms :: artifacts dl 13ms
        :: modules in use:
        net.jpountz.lz4#lz4;1.3.0 from central in [default]
        org.apache.kafka#kafka-clients;0.10.0.1 from central in [default]
        org.apache.spark#spark-sql-kafka-0-10_2.11;2.3.4 from central in [default]
        org.slf4j#slf4j-api;1.7.16 from central in [default]
        org.spark-project.spark#unused;1.0.0 from central in [default]
        org.xerial.snappy#snappy-java;1.1.2.6 from central in [default]
        ---------------------------------------------------------------------
        |                  |            modules            ||   artifacts   |
        |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
        ---------------------------------------------------------------------
        |      default     |   6   |   0   |   0   |   0   ||   6   |   0   |
        ---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-61ec7441-86a5-4a38-81b1-2153065117f4
        confs: [default]
        0 artifacts copied, 6 already retrieved (0kB/13ms)
2020-09-21 03:38:32 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-09-21 03:38:33 INFO  SparkContext:54 - Running Spark version 2.3.4
2020-09-21 03:38:33 INFO  SparkContext:54 - Submitted application: StructuredStreamingSetup
2020-09-21 03:38:33 INFO  SecurityManager:54 - Changing view acls to: root
2020-09-21 03:38:33 INFO  SecurityManager:54 - Changing modify acls to: root
2020-09-21 03:38:33 INFO  SecurityManager:54 - Changing view acls groups to: 
2020-09-21 03:38:33 INFO  SecurityManager:54 - Changing modify acls groups to: 
2020-09-21 03:38:33 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
2020-09-21 03:38:34 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 45315.
2020-09-21 03:38:34 INFO  SparkEnv:54 - Registering MapOutputTracker
2020-09-21 03:38:34 INFO  SparkEnv:54 - Registering BlockManagerMaster
2020-09-21 03:38:34 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-09-21 03:38:34 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up
2020-09-21 03:38:34 INFO  DiskBlockManager:54 - Created local directory at /tmp/blockmgr-357c7fb4-a779-4924-a000-facfe367f35b
2020-09-21 03:38:34 INFO  MemoryStore:54 - MemoryStore started with capacity 366.3 MB
2020-09-21 03:38:34 INFO  SparkEnv:54 - Registering OutputCommitCoordinator
2020-09-21 03:38:34 INFO  log:192 - Logging initialized @4079ms
2020-09-21 03:38:34 INFO  Server:351 - jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2020-09-21 03:38:34 INFO  Server:419 - Started @4205ms
2020-09-21 03:38:34 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2020-09-21 03:38:34 INFO  AbstractConnector:278 - Started ServerConnector@1e60c5cc{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
2020-09-21 03:38:34 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4041.
2020-09-21 03:38:34 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6796903c{/jobs,null,AVAILABLE,@Spark}
2020-09-21 03:38:34 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2de7f4eb{/jobs/json,null,AVAILABLE,@Spark}
2020-09-21 03:38:34 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5053811e{/jobs/job,null,AVAILABLE,@Spark}
2020-09-21 03:38:34 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@761ac61{/jobs/job/json,null,AVAILABLE,@Spark}
2020-09-21 03:38:34 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@153340ca{/stages,null,AVAILABLE,@Spark}
2020-09-21 03:38:34 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@59246803{/stages/json,null,AVAILABLE,@Spark}
2020-09-21 03:38:34 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4b426f46{/stages/stage,null,AVAILABLE,@Spark}
2020-09-21 03:38:34 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@34eee0b5{/stages/stage/json,null,AVAILABLE,@Spark}
2020-09-21 03:38:34 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6c8e9131{/stages/pool,null,AVAILABLE,@Spark}
2020-09-21 03:38:34 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7bd459d3{/stages/pool/json,null,AVAILABLE,@Spark}
2020-09-21 03:38:34 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3205f39c{/storage,null,AVAILABLE,@Spark}
2020-09-21 03:38:34 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@712d4621{/storage/json,null,AVAILABLE,@Spark}
2020-09-21 03:38:34 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@69b2562c{/storage/rdd,null,AVAILABLE,@Spark}
2020-09-21 03:38:34 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@30f19008{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-09-21 03:38:34 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2df50c99{/environment,null,AVAILABLE,@Spark}
2020-09-21 03:38:34 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@36f25b48{/environment/json,null,AVAILABLE,@Spark}
2020-09-21 03:38:34 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4d8a688c{/executors,null,AVAILABLE,@Spark}
2020-09-21 03:38:34 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@71c7c0e5{/executors/json,null,AVAILABLE,@Spark}
2020-09-21 03:38:34 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@74d39b80{/executors/threadDump,null,AVAILABLE,@Spark}
2020-09-21 03:38:34 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@505fb649{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-09-21 03:38:34 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4e8f7ac6{/static,null,AVAILABLE,@Spark}
2020-09-21 03:38:34 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1e58fe5a{/,null,AVAILABLE,@Spark}
2020-09-21 03:38:34 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@75d7f77f{/api,null,AVAILABLE,@Spark}
2020-09-21 03:38:34 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2ff8984b{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-09-21 03:38:34 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@59c0d17b{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-09-21 03:38:34 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://81c68c0b4a46:4041
2020-09-21 03:38:34 INFO  SparkContext:54 - Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.11-2.3.4.jar at spark://81c68c0b4a46:45315/jars/org.apache.spark_spark-sql-kafka-0-10_2.11-2.3.4.jar with timestamp 1600659514714
2020-09-21 03:38:34 INFO  SparkContext:54 - Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.10.0.1.jar at spark://81c68c0b4a46:45315/jars/org.apache.kafka_kafka-clients-0.10.0.1.jar with timestamp 1600659514717
2020-09-21 03:38:34 INFO  SparkContext:54 - Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://81c68c0b4a46:45315/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1600659514718
2020-09-21 03:38:34 INFO  SparkContext:54 - Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.3.0.jar at spark://81c68c0b4a46:45315/jars/net.jpountz.lz4_lz4-1.3.0.jar with timestamp 1600659514719
2020-09-21 03:38:34 INFO  SparkContext:54 - Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.2.6.jar at spark://81c68c0b4a46:45315/jars/org.xerial.snappy_snappy-java-1.1.2.6.jar with timestamp 1600659514720
2020-09-21 03:38:34 INFO  SparkContext:54 - Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://81c68c0b4a46:45315/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1600659514721
2020-09-21 03:38:34 INFO  SparkContext:54 - Added file file:/home/workspace/progress_reporter.py at file:/home/workspace/progress_reporter.py with timestamp 1600659514755
2020-09-21 03:38:34 INFO  Utils:54 - Copying /home/workspace/progress_reporter.py to /tmp/spark-e7c4de71-15d7-4496-b41e-fb7daf61aae6/userFiles-334e2172-2130-4078-b9fb-17f33a420d6d/progress_reporter.py
2020-09-21 03:38:34 INFO  SparkContext:54 - Added file file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.11-2.3.4.jar at file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.11-2.3.4.jar with timestamp 1600659514773
2020-09-21 03:38:34 INFO  Utils:54 - Copying /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.11-2.3.4.jar to /tmp/spark-e7c4de71-15d7-4496-b41e-fb7daf61aae6/userFiles-334e2172-2130-4078-b9fb-17f33a420d6d/org.apache.spark_spark-sql-kafka-0-10_2.11-2.3.4.jar
2020-09-21 03:38:34 INFO  SparkContext:54 - Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.10.0.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.10.0.1.jar with timestamp 1600659514787
2020-09-21 03:38:34 INFO  Utils:54 - Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.10.0.1.jar to /tmp/spark-e7c4de71-15d7-4496-b41e-fb7daf61aae6/userFiles-334e2172-2130-4078-b9fb-17f33a420d6d/org.apache.kafka_kafka-clients-0.10.0.1.jar
2020-09-21 03:38:34 INFO  SparkContext:54 - Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1600659514801
2020-09-21 03:38:34 INFO  Utils:54 - Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-e7c4de71-15d7-4496-b41e-fb7daf61aae6/userFiles-334e2172-2130-4078-b9fb-17f33a420d6d/org.spark-project.spark_unused-1.0.0.jar
2020-09-21 03:38:34 INFO  SparkContext:54 - Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.3.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.3.0.jar with timestamp 1600659514815
2020-09-21 03:38:34 INFO  Utils:54 - Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.3.0.jar to /tmp/spark-e7c4de71-15d7-4496-b41e-fb7daf61aae6/userFiles-334e2172-2130-4078-b9fb-17f33a420d6d/net.jpountz.lz4_lz4-1.3.0.jar
2020-09-21 03:38:34 INFO  SparkContext:54 - Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.2.6.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.2.6.jar with timestamp 1600659514826
2020-09-21 03:38:34 INFO  Utils:54 - Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.2.6.jar to /tmp/spark-e7c4de71-15d7-4496-b41e-fb7daf61aae6/userFiles-334e2172-2130-4078-b9fb-17f33a420d6d/org.xerial.snappy_snappy-java-1.1.2.6.jar
2020-09-21 03:38:34 INFO  SparkContext:54 - Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1600659514835
2020-09-21 03:38:34 INFO  Utils:54 - Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-e7c4de71-15d7-4496-b41e-fb7daf61aae6/userFiles-334e2172-2130-4078-b9fb-17f33a420d6d/org.slf4j_slf4j-api-1.7.16.jar
2020-09-21 03:38:34 INFO  Executor:54 - Starting executor ID driver on host localhost
2020-09-21 03:38:34 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34727.
2020-09-21 03:38:34 INFO  NettyBlockTransferService:54 - Server created on 81c68c0b4a46:34727
2020-09-21 03:38:34 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-09-21 03:38:34 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, 81c68c0b4a46, 34727, None)
2020-09-21 03:38:34 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 81c68c0b4a46:34727 with 366.3 MB RAM, BlockManagerId(driver, 81c68c0b4a46, 34727, None)
2020-09-21 03:38:35 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, 81c68c0b4a46, 34727, None)
2020-09-21 03:38:35 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, 81c68c0b4a46, 34727, None)
2020-09-21 03:38:35 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1f771615{/metrics/json,null,AVAILABLE,@Spark}
2020-09-21 03:38:35 INFO  SharedState:54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/workspace/spark-warehouse/').
2020-09-21 03:38:35 INFO  SharedState:54 - Warehouse path is 'file:/home/workspace/spark-warehouse/'.
2020-09-21 03:38:35 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5de565e6{/SQL,null,AVAILABLE,@Spark}
2020-09-21 03:38:35 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@36aa0b99{/SQL/json,null,AVAILABLE,@Spark}
2020-09-21 03:38:35 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7f4268a5{/SQL/execution,null,AVAILABLE,@Spark}
2020-09-21 03:38:35 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5365a2a3{/SQL/execution/json,null,AVAILABLE,@Spark}
2020-09-21 03:38:35 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@280791cd{/static/sql,null,AVAILABLE,@Spark}
2020-09-21 03:38:36 INFO  StateStoreCoordinatorRef:54 - Registered StateStoreCoordinator endpoint
2020-09-21 03:38:36 INFO  ConsumerConfig:178 - ConsumerConfig values: 
        metric.reporters = []
        metadata.max.age.ms = 300000
        partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
        reconnect.backoff.ms = 50
        sasl.kerberos.ticket.renew.window.factor = 0.8
        max.partition.fetch.bytes = 1048576
        bootstrap.servers = [localhost:9092]
        ssl.keystore.type = JKS
        enable.auto.commit = false
        sasl.mechanism = GSSAPI
        interceptor.classes = null
        exclude.internal.topics = true
        ssl.truststore.password = null
        client.id = 
        ssl.endpoint.identification.algorithm = null
        max.poll.records = 1
        check.crcs = true
        request.timeout.ms = 40000
        heartbeat.interval.ms = 3000
        auto.commit.interval.ms = 5000
        receive.buffer.bytes = 65536
        ssl.truststore.type = JKS
        ssl.truststore.location = null
        ssl.keystore.password = null
        fetch.min.bytes = 1
        send.buffer.bytes = 131072
        value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
        group.id = spark-kafka-source-67595463-8258-4ad4-8957-0cb7905d7cc0-1787544151-driver-0
        retry.backoff.ms = 100
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        ssl.trustmanager.algorithm = PKIX
        ssl.key.password = null
        fetch.max.wait.ms = 500
        sasl.kerberos.min.time.before.relogin = 60000
        connections.max.idle.ms = 540000
        session.timeout.ms = 30000
        metrics.num.samples = 2
        key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
        ssl.protocol = TLS
        ssl.provider = null
        ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
        ssl.keystore.location = null
        ssl.cipher.suites = null
        security.protocol = PLAINTEXT
        ssl.keymanager.algorithm = SunX509
        metrics.sample.window.ms = 30000
        auto.offset.reset = earliest

2020-09-21 03:38:36 INFO  ConsumerConfig:178 - ConsumerConfig values: 
        metric.reporters = []
        metadata.max.age.ms = 300000
        partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
        reconnect.backoff.ms = 50
        sasl.kerberos.ticket.renew.window.factor = 0.8
        max.partition.fetch.bytes = 1048576
        bootstrap.servers = [localhost:9092]
        ssl.keystore.type = JKS
        enable.auto.commit = false
        sasl.mechanism = GSSAPI
        interceptor.classes = null
        exclude.internal.topics = true
        ssl.truststore.password = null
        client.id = consumer-1
        ssl.endpoint.identification.algorithm = null
        max.poll.records = 1
        check.crcs = true
        request.timeout.ms = 40000
        heartbeat.interval.ms = 3000
        auto.commit.interval.ms = 5000
        receive.buffer.bytes = 65536
        ssl.truststore.type = JKS
        ssl.truststore.location = null
        ssl.keystore.password = null
        fetch.min.bytes = 1
        send.buffer.bytes = 131072
        value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
        group.id = spark-kafka-source-67595463-8258-4ad4-8957-0cb7905d7cc0-1787544151-driver-0
        retry.backoff.ms = 100
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        ssl.trustmanager.algorithm = PKIX
        ssl.key.password = null
        fetch.max.wait.ms = 500
        sasl.kerberos.min.time.before.relogin = 60000
        connections.max.idle.ms = 540000
        session.timeout.ms = 30000
        metrics.num.samples = 2
        key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
        ssl.protocol = TLS
        ssl.provider = null
        ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
        ssl.keystore.location = null
        ssl.cipher.suites = null
        security.protocol = PLAINTEXT
        ssl.keymanager.algorithm = SunX509
        metrics.sample.window.ms = 30000
        auto.offset.reset = earliest

2020-09-21 03:38:36 INFO  AppInfoParser:83 - Kafka version : 0.10.0.1
2020-09-21 03:38:36 INFO  AppInfoParser:84 - Kafka commitId : a7a17cdec9eaa6c5
root
 |-- key: binary (nullable = true)
 |-- value: binary (nullable = true)
 |-- topic: string (nullable = true)
 |-- partition: integer (nullable = true)
 |-- offset: long (nullable = true)
 |-- timestamp: timestamp (nullable = true)
 |-- timestampType: integer (nullable = true)

2020-09-21 03:38:38 INFO  MicroBatchExecution:54 - Starting [id = 721ff79a-b9e8-4fff-9fe6-1e60464dcc0e, runId = d37563d8-eff3-48cb-a56f-7837bbe993aa]. Use file:///tmp/temporary-86f82092-edad-465d-b5bb-d79047f4d17f to store the query checkpoint.
2020-09-21 03:38:38 INFO  AbstractConnector:318 - Stopped Spark@1e60c5cc{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
2020-09-21 03:38:38 INFO  SparkUI:54 - Stopped Spark web UI at http://81c68c0b4a46:4041
2020-09-21 03:38:38 INFO  ConsumerConfig:178 - ConsumerConfig values: 
        metric.reporters = []
        metadata.max.age.ms = 300000
        partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
        reconnect.backoff.ms = 50
        sasl.kerberos.ticket.renew.window.factor = 0.8
        max.partition.fetch.bytes = 1048576
        bootstrap.servers = [localhost:9092]
        ssl.keystore.type = JKS
        enable.auto.commit = false
        sasl.mechanism = GSSAPI
        interceptor.classes = null
        exclude.internal.topics = true
        ssl.truststore.password = null
        client.id = 
        ssl.endpoint.identification.algorithm = null
        max.poll.records = 1
        check.crcs = true
        request.timeout.ms = 40000
        heartbeat.interval.ms = 3000
        auto.commit.interval.ms = 5000
        receive.buffer.bytes = 65536
        ssl.truststore.type = JKS
        ssl.truststore.location = null
        ssl.keystore.password = null
        fetch.min.bytes = 1
        send.buffer.bytes = 131072
        value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
        group.id = spark-kafka-source-8c0e2c79-0133-4614-a806-77ab4e5353f5-383080297-driver-0
        retry.backoff.ms = 100
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        ssl.trustmanager.algorithm = PKIX
        ssl.key.password = null
        fetch.max.wait.ms = 500
        sasl.kerberos.min.time.before.relogin = 60000
        connections.max.idle.ms = 540000
        session.timeout.ms = 30000
        metrics.num.samples = 2
        key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
        ssl.protocol = TLS
        ssl.provider = null
        ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
        ssl.keystore.location = null
        ssl.cipher.suites = null
        security.protocol = PLAINTEXT
        ssl.keymanager.algorithm = SunX509
        metrics.sample.window.ms = 30000
        auto.offset.reset = earliest

2020-09-21 03:38:38 INFO  ConsumerConfig:178 - ConsumerConfig values: 
        metric.reporters = []
        metadata.max.age.ms = 300000
        partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
        reconnect.backoff.ms = 50
        sasl.kerberos.ticket.renew.window.factor = 0.8
        max.partition.fetch.bytes = 1048576
        bootstrap.servers = [localhost:9092]
        ssl.keystore.type = JKS
        enable.auto.commit = false
        sasl.mechanism = GSSAPI
        interceptor.classes = null
        exclude.internal.topics = true
        ssl.truststore.password = null
        client.id = consumer-2
        ssl.endpoint.identification.algorithm = null
        max.poll.records = 1
        check.crcs = true
        request.timeout.ms = 40000
        heartbeat.interval.ms = 3000
        auto.commit.interval.ms = 5000
        receive.buffer.bytes = 65536
        ssl.truststore.type = JKS
        ssl.truststore.location = null
        ssl.keystore.password = null
        fetch.min.bytes = 1
        send.buffer.bytes = 131072
        value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
        group.id = spark-kafka-source-8c0e2c79-0133-4614-a806-77ab4e5353f5-383080297-driver-0
        retry.backoff.ms = 100
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        ssl.trustmanager.algorithm = PKIX
        ssl.key.password = null
        fetch.max.wait.ms = 500
        sasl.kerberos.min.time.before.relogin = 60000
        connections.max.idle.ms = 540000
        session.timeout.ms = 30000
        metrics.num.samples = 2
        key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
        ssl.protocol = TLS
        ssl.provider = null
        ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
        ssl.keystore.location = null
        ssl.cipher.suites = null
        security.protocol = PLAINTEXT
        ssl.keymanager.algorithm = SunX509
        metrics.sample.window.ms = 30000
        auto.offset.reset = earliest

2020-09-21 03:38:38 INFO  AppInfoParser:83 - Kafka version : 0.10.0.1
2020-09-21 03:38:38 INFO  AppInfoParser:84 - Kafka commitId : a7a17cdec9eaa6c5
2020-09-21 03:38:39 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!
2020-09-21 03:38:39 INFO  MemoryStore:54 - MemoryStore cleared
2020-09-21 03:38:39 INFO  BlockManager:54 - BlockManager stopped
2020-09-21 03:38:39 ERROR MicroBatchExecution:91 - Query [id = 721ff79a-b9e8-4fff-9fe6-1e60464dcc0e, runId = d37563d8-eff3-48cb-a56f-7837bbe993aa] terminated with error
java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
java.lang.reflect.Constructor.newInstance(Constructor.java:423)
py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
py4j.Gateway.invoke(Gateway.java:238)
py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
py4j.GatewayConnection.run(GatewayConnection.java:238)
java.lang.Thread.run(Thread.java:748)

The currently active SparkContext was created at:

org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
java.lang.reflect.Constructor.newInstance(Constructor.java:423)
py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
py4j.Gateway.invoke(Gateway.java:238)
py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
py4j.GatewayConnection.run(GatewayConnection.java:238)
java.lang.Thread.run(Thread.java:748)
         
        at org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:99)
        at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:91)
        at org.apache.spark.sql.SparkSession.cloneSession(SparkSession.scala:256)
        at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:268)
        at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:189)
2020-09-21 03:38:39 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped
2020-09-21 03:38:39 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!
2020-09-21 03:38:39 INFO  SparkContext:54 - Successfully stopped SparkContext
2020-09-21 03:38:39 WARN  NettyRpcEnv:66 - Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@6824a8c6 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@75e8d8f4[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]
Exception in thread "stream execution thread for [id = 721ff79a-b9e8-4fff-9fe6-1e60464dcc0e, runId = d37563d8-eff3-48cb-a56f-7837bbe993aa]" org.apache.spark.SparkException: Exception thrown in awaitResult: 
        at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:205)
        at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
        at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:92)
        at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:76)
        at org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef.deactivateInstances(StateStoreCoordinator.scala:108)
        at org.apache.spark.sql.streaming.StreamingQueryManager.notifyQueryTermination(StreamingQueryManager.scala:357)
        at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runStream$2.apply(StreamExecution.scala:327)
        at org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)
        at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:308)
        at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:189)
Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.
        at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:158)
        at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:135)
        at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:229)
        at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:523)
        at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:91)
        ... 7 more
2020-09-21 03:38:39 INFO  ShutdownHookManager:54 - Shutdown hook called
2020-09-21 03:38:39 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-d5db894b-ea7a-4647-acf3-b46703797fae
2020-09-21 03:38:39 INFO  ShutdownHookManager:54 - Deleting directory /tmp/temporary-86f82092-edad-465d-b5bb-d79047f4d17f
2020-09-21 03:38:39 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-e7c4de71-15d7-4496-b41e-fb7daf61aae6
2020-09-21 03:38:39 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-e7c4de71-15d7-4496-b41e-fb7daf61aae6/pyspark-da9b093f-bd00-446a-8fc9-f7b20594b7be
2020-09-21 03:38:39 INFO  ShutdownHookManager:54 - Deleting directory /tmp/temporaryReader-99b8861a-2879-47ea-8bb7-ea268f22d5fb