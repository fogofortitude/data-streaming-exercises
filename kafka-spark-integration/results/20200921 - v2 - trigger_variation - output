root@81c68c0b4a46:/home/workspace# spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.4 trigger_variation.py
Ivy Default Cache set to: /root/.ivy2/cache
The jars for the packages stored in: /root/.ivy2/jars
:: loading settings :: url = jar:file:/opt/spark-2.3.4-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
org.apache.spark#spark-sql-kafka-0-10_2.11 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-ab6c76a6-b290-4d1a-95dd-f67b26b36197;1.0
        confs: [default]
        found org.apache.spark#spark-sql-kafka-0-10_2.11;2.3.4 in central
        found org.apache.kafka#kafka-clients;0.10.0.1 in central
        found net.jpountz.lz4#lz4;1.3.0 in central
        found org.xerial.snappy#snappy-java;1.1.2.6 in central
        found org.slf4j#slf4j-api;1.7.16 in central
        found org.spark-project.spark#unused;1.0.0 in central
:: resolution report :: resolve 613ms :: artifacts dl 14ms
        :: modules in use:
        net.jpountz.lz4#lz4;1.3.0 from central in [default]
        org.apache.kafka#kafka-clients;0.10.0.1 from central in [default]
        org.apache.spark#spark-sql-kafka-0-10_2.11;2.3.4 from central in [default]
        org.slf4j#slf4j-api;1.7.16 from central in [default]
        org.spark-project.spark#unused;1.0.0 from central in [default]
        org.xerial.snappy#snappy-java;1.1.2.6 from central in [default]
        ---------------------------------------------------------------------
        |                  |            modules            ||   artifacts   |
        |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
        ---------------------------------------------------------------------
        |      default     |   6   |   0   |   0   |   0   ||   6   |   0   |
        ---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-ab6c76a6-b290-4d1a-95dd-f67b26b36197
        confs: [default]
        0 artifacts copied, 6 already retrieved (0kB/16ms)
2020-09-21 01:55:08 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-09-21 01:55:09 INFO  SparkContext:54 - Running Spark version 2.3.4
2020-09-21 01:55:09 INFO  SparkContext:54 - Submitted application: StructuredStreamingSetup
2020-09-21 01:55:09 INFO  SecurityManager:54 - Changing view acls to: root
2020-09-21 01:55:09 INFO  SecurityManager:54 - Changing modify acls to: root
2020-09-21 01:55:09 INFO  SecurityManager:54 - Changing view acls groups to: 
2020-09-21 01:55:09 INFO  SecurityManager:54 - Changing modify acls groups to: 
2020-09-21 01:55:09 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
2020-09-21 01:55:09 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 45553.
2020-09-21 01:55:10 INFO  SparkEnv:54 - Registering MapOutputTracker
2020-09-21 01:55:10 INFO  SparkEnv:54 - Registering BlockManagerMaster
2020-09-21 01:55:10 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2020-09-21 01:55:10 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up
2020-09-21 01:55:10 INFO  DiskBlockManager:54 - Created local directory at /tmp/blockmgr-f871e09c-ec0c-417d-92b3-cd666fa29ede
2020-09-21 01:55:10 INFO  MemoryStore:54 - MemoryStore started with capacity 366.3 MB
2020-09-21 01:55:10 INFO  SparkEnv:54 - Registering OutputCommitCoordinator
2020-09-21 01:55:10 INFO  log:192 - Logging initialized @4233ms
2020-09-21 01:55:10 INFO  Server:351 - jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2020-09-21 01:55:10 INFO  Server:419 - Started @4393ms
2020-09-21 01:55:10 INFO  AbstractConnector:278 - Started ServerConnector@1e60c5cc{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-09-21 01:55:10 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4040.
2020-09-21 01:55:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@12baa57{/jobs,null,AVAILABLE,@Spark}
2020-09-21 01:55:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6882b970{/jobs/json,null,AVAILABLE,@Spark}
2020-09-21 01:55:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@191fdbe6{/jobs/job,null,AVAILABLE,@Spark}
2020-09-21 01:55:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2de7f4eb{/jobs/job/json,null,AVAILABLE,@Spark}
2020-09-21 01:55:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5053811e{/stages,null,AVAILABLE,@Spark}
2020-09-21 01:55:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@37f4b019{/stages/json,null,AVAILABLE,@Spark}
2020-09-21 01:55:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@761ac61{/stages/stage,null,AVAILABLE,@Spark}
2020-09-21 01:55:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4b426f46{/stages/stage/json,null,AVAILABLE,@Spark}
2020-09-21 01:55:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@414ac0eb{/stages/pool,null,AVAILABLE,@Spark}
2020-09-21 01:55:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4ef0ed18{/stages/pool/json,null,AVAILABLE,@Spark}
2020-09-21 01:55:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@34eee0b5{/storage,null,AVAILABLE,@Spark}
2020-09-21 01:55:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6c8e9131{/storage/json,null,AVAILABLE,@Spark}
2020-09-21 01:55:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7bd459d3{/storage/rdd,null,AVAILABLE,@Spark}
2020-09-21 01:55:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3205f39c{/storage/rdd/json,null,AVAILABLE,@Spark}
2020-09-21 01:55:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@712d4621{/environment,null,AVAILABLE,@Spark}
2020-09-21 01:55:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@69b2562c{/environment/json,null,AVAILABLE,@Spark}
2020-09-21 01:55:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@30f19008{/executors,null,AVAILABLE,@Spark}
2020-09-21 01:55:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2df50c99{/executors/json,null,AVAILABLE,@Spark}
2020-09-21 01:55:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@36f25b48{/executors/threadDump,null,AVAILABLE,@Spark}
2020-09-21 01:55:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4d8a688c{/executors/threadDump/json,null,AVAILABLE,@Spark}
2020-09-21 01:55:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@71c7c0e5{/static,null,AVAILABLE,@Spark}
2020-09-21 01:55:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6c90fe90{/,null,AVAILABLE,@Spark}
2020-09-21 01:55:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@15eea8b0{/api,null,AVAILABLE,@Spark}
2020-09-21 01:55:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@151c7a5d{/jobs/job/kill,null,AVAILABLE,@Spark}
2020-09-21 01:55:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3cad7471{/stages/stage/kill,null,AVAILABLE,@Spark}
2020-09-21 01:55:10 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://81c68c0b4a46:4040
2020-09-21 01:55:10 INFO  SparkContext:54 - Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.11-2.3.4.jar at spark://81c68c0b4a46:45553/jars/org.apache.spark_spark-sql-kafka-0-10_2.11-2.3.4.jar with timestamp 1600653310756
2020-09-21 01:55:10 INFO  SparkContext:54 - Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.10.0.1.jar at spark://81c68c0b4a46:45553/jars/org.apache.kafka_kafka-clients-0.10.0.1.jar with timestamp 1600653310759
2020-09-21 01:55:10 INFO  SparkContext:54 - Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://81c68c0b4a46:45553/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1600653310760
2020-09-21 01:55:10 INFO  SparkContext:54 - Added JAR file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.3.0.jar at spark://81c68c0b4a46:45553/jars/net.jpountz.lz4_lz4-1.3.0.jar with timestamp 1600653310761
2020-09-21 01:55:10 INFO  SparkContext:54 - Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.2.6.jar at spark://81c68c0b4a46:45553/jars/org.xerial.snappy_snappy-java-1.1.2.6.jar with timestamp 1600653310762
2020-09-21 01:55:10 INFO  SparkContext:54 - Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at spark://81c68c0b4a46:45553/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1600653310763
2020-09-21 01:55:10 INFO  SparkContext:54 - Added file file:/home/workspace/trigger_variation.py at file:/home/workspace/trigger_variation.py with timestamp 1600653310798
2020-09-21 01:55:10 INFO  Utils:54 - Copying /home/workspace/trigger_variation.py to /tmp/spark-ac14f8c9-ec93-42a6-a743-34f5aa3dd7cb/userFiles-9001027a-4fb3-4e5e-9bd6-664e5f798bb0/trigger_variation.py
2020-09-21 01:55:10 INFO  SparkContext:54 - Added file file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.11-2.3.4.jar at file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.11-2.3.4.jar with timestamp 1600653310820
2020-09-21 01:55:10 INFO  Utils:54 - Copying /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.11-2.3.4.jar to /tmp/spark-ac14f8c9-ec93-42a6-a743-34f5aa3dd7cb/userFiles-9001027a-4fb3-4e5e-9bd6-664e5f798bb0/org.apache.spark_spark-sql-kafka-0-10_2.11-2.3.4.jar
2020-09-21 01:55:10 INFO  SparkContext:54 - Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.10.0.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-0.10.0.1.jar with timestamp 1600653310834
2020-09-21 01:55:10 INFO  Utils:54 - Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-0.10.0.1.jar to /tmp/spark-ac14f8c9-ec93-42a6-a743-34f5aa3dd7cb/userFiles-9001027a-4fb3-4e5e-9bd6-664e5f798bb0/org.apache.kafka_kafka-clients-0.10.0.1.jar
2020-09-21 01:55:10 INFO  SparkContext:54 - Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1600653310847
2020-09-21 01:55:10 INFO  Utils:54 - Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-ac14f8c9-ec93-42a6-a743-34f5aa3dd7cb/userFiles-9001027a-4fb3-4e5e-9bd6-664e5f798bb0/org.spark-project.spark_unused-1.0.0.jar
2020-09-21 01:55:10 INFO  SparkContext:54 - Added file file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.3.0.jar at file:///root/.ivy2/jars/net.jpountz.lz4_lz4-1.3.0.jar with timestamp 1600653310860
2020-09-21 01:55:10 INFO  Utils:54 - Copying /root/.ivy2/jars/net.jpountz.lz4_lz4-1.3.0.jar to /tmp/spark-ac14f8c9-ec93-42a6-a743-34f5aa3dd7cb/userFiles-9001027a-4fb3-4e5e-9bd6-664e5f798bb0/net.jpountz.lz4_lz4-1.3.0.jar
2020-09-21 01:55:10 INFO  SparkContext:54 - Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.2.6.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.2.6.jar with timestamp 1600653310874
2020-09-21 01:55:10 INFO  Utils:54 - Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.2.6.jar to /tmp/spark-ac14f8c9-ec93-42a6-a743-34f5aa3dd7cb/userFiles-9001027a-4fb3-4e5e-9bd6-664e5f798bb0/org.xerial.snappy_snappy-java-1.1.2.6.jar
2020-09-21 01:55:10 INFO  SparkContext:54 - Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar with timestamp 1600653310888
2020-09-21 01:55:10 INFO  Utils:54 - Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar to /tmp/spark-ac14f8c9-ec93-42a6-a743-34f5aa3dd7cb/userFiles-9001027a-4fb3-4e5e-9bd6-664e5f798bb0/org.slf4j_slf4j-api-1.7.16.jar
2020-09-21 01:55:10 INFO  Executor:54 - Starting executor ID driver on host localhost
2020-09-21 01:55:11 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45601.
2020-09-21 01:55:11 INFO  NettyBlockTransferService:54 - Server created on 81c68c0b4a46:45601
2020-09-21 01:55:11 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2020-09-21 01:55:11 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, 81c68c0b4a46, 45601, None)
2020-09-21 01:55:11 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 81c68c0b4a46:45601 with 366.3 MB RAM, BlockManagerId(driver, 81c68c0b4a46, 45601, None)
2020-09-21 01:55:11 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, 81c68c0b4a46, 45601, None)
2020-09-21 01:55:11 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, 81c68c0b4a46, 45601, None)
2020-09-21 01:55:11 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@59defd57{/metrics/json,null,AVAILABLE,@Spark}
2020-09-21 01:55:11 INFO  SharedState:54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/workspace/spark-warehouse/').
2020-09-21 01:55:11 INFO  SharedState:54 - Warehouse path is 'file:/home/workspace/spark-warehouse/'.
2020-09-21 01:55:11 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@8190a67{/SQL,null,AVAILABLE,@Spark}
2020-09-21 01:55:11 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@46a11ea2{/SQL/json,null,AVAILABLE,@Spark}
2020-09-21 01:55:11 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@780cacae{/SQL/execution,null,AVAILABLE,@Spark}
2020-09-21 01:55:11 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6f4dd917{/SQL/execution/json,null,AVAILABLE,@Spark}
2020-09-21 01:55:11 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@17f6408a{/static/sql,null,AVAILABLE,@Spark}
2020-09-21 01:55:12 INFO  StateStoreCoordinatorRef:54 - Registered StateStoreCoordinator endpoint
2020-09-21 01:55:12 INFO  ConsumerConfig:178 - ConsumerConfig values: 
        metric.reporters = []
        metadata.max.age.ms = 300000
        partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
        reconnect.backoff.ms = 50
        sasl.kerberos.ticket.renew.window.factor = 0.8
        max.partition.fetch.bytes = 1048576
        bootstrap.servers = [localhost:9092]
        ssl.keystore.type = JKS
        enable.auto.commit = false
        sasl.mechanism = GSSAPI
        interceptor.classes = null
        exclude.internal.topics = true
        ssl.truststore.password = null
        client.id = 
        ssl.endpoint.identification.algorithm = null
        max.poll.records = 1
        check.crcs = true
        request.timeout.ms = 40000
        heartbeat.interval.ms = 3000
        auto.commit.interval.ms = 5000
        receive.buffer.bytes = 65536
        ssl.truststore.type = JKS
        ssl.truststore.location = null
        ssl.keystore.password = null
        fetch.min.bytes = 1
        send.buffer.bytes = 131072
        value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
        group.id = spark-kafka-source-8c2c1c1e-b780-4469-802f-39fb267ac675-1857260769-driver-0
        retry.backoff.ms = 100
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        ssl.trustmanager.algorithm = PKIX
        ssl.key.password = null
        fetch.max.wait.ms = 500
        sasl.kerberos.min.time.before.relogin = 60000
        connections.max.idle.ms = 540000
        session.timeout.ms = 30000
        metrics.num.samples = 2
        key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
        ssl.protocol = TLS
        ssl.provider = null
        ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
        ssl.keystore.location = null
        ssl.cipher.suites = null
        security.protocol = PLAINTEXT
        ssl.keymanager.algorithm = SunX509
        metrics.sample.window.ms = 30000
        auto.offset.reset = earliest

2020-09-21 01:55:12 INFO  ConsumerConfig:178 - ConsumerConfig values: 
        metric.reporters = []
        metadata.max.age.ms = 300000
        partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
        reconnect.backoff.ms = 50
        sasl.kerberos.ticket.renew.window.factor = 0.8
        max.partition.fetch.bytes = 1048576
        bootstrap.servers = [localhost:9092]
        ssl.keystore.type = JKS
        enable.auto.commit = false
        sasl.mechanism = GSSAPI
        interceptor.classes = null
        exclude.internal.topics = true
        ssl.truststore.password = null
        client.id = consumer-1
        ssl.endpoint.identification.algorithm = null
        max.poll.records = 1
        check.crcs = true
        request.timeout.ms = 40000
        heartbeat.interval.ms = 3000
        auto.commit.interval.ms = 5000
        receive.buffer.bytes = 65536
        ssl.truststore.type = JKS
        ssl.truststore.location = null
        ssl.keystore.password = null
        fetch.min.bytes = 1
        send.buffer.bytes = 131072
        value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
        group.id = spark-kafka-source-8c2c1c1e-b780-4469-802f-39fb267ac675-1857260769-driver-0
        retry.backoff.ms = 100
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        ssl.trustmanager.algorithm = PKIX
        ssl.key.password = null
        fetch.max.wait.ms = 500
        sasl.kerberos.min.time.before.relogin = 60000
        connections.max.idle.ms = 540000
        session.timeout.ms = 30000
        metrics.num.samples = 2
        key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
        ssl.protocol = TLS
        ssl.provider = null
        ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
        ssl.keystore.location = null
        ssl.cipher.suites = null
        security.protocol = PLAINTEXT
        ssl.keymanager.algorithm = SunX509
        metrics.sample.window.ms = 30000
        auto.offset.reset = earliest

2020-09-21 01:55:12 INFO  AppInfoParser:83 - Kafka version : 0.10.0.1
2020-09-21 01:55:12 INFO  AppInfoParser:84 - Kafka commitId : a7a17cdec9eaa6c5
root
 |-- key: binary (nullable = true)
 |-- value: binary (nullable = true)
 |-- topic: string (nullable = true)
 |-- partition: integer (nullable = true)
 |-- offset: long (nullable = true)
 |-- timestamp: timestamp (nullable = true)
 |-- timestampType: integer (nullable = true)

Traceback (most recent call last):
  File "/opt/spark-2.3.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
  File "/opt/spark-2.3.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.sql.streaming.Trigger.ProcessingTime.
: java.lang.IllegalArgumentException: Invalid interval: 20
        at org.apache.spark.unsafe.types.CalendarInterval.fromCaseInsensitiveString(CalendarInterval.java:110)
        at org.apache.spark.sql.streaming.ProcessingTime$.apply(ProcessingTime.scala:77)
        at org.apache.spark.sql.streaming.ProcessingTime.apply(ProcessingTime.scala)
        at org.apache.spark.sql.streaming.Trigger.ProcessingTime(Trigger.java:87)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:748)


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/workspace/trigger_variation.py", line 43, in <module>
    run_spark_job(spark)
  File "/home/workspace/trigger_variation.py", line 24, in run_spark_job
    .trigger(processingTime="20") \
  File "/opt/spark-2.3.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/__init__.py", line 105, in wrapper
  File "/opt/spark-2.3.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/streaming.py", line 824, in trigger
  File "/opt/spark-2.3.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
  File "/opt/spark-2.3.4-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/utils.py", line 79, in deco
pyspark.sql.utils.IllegalArgumentException: 'Invalid interval: 20'
2020-09-21 01:55:14 INFO  SparkContext:54 - Invoking stop() from shutdown hook
2020-09-21 01:55:14 INFO  AbstractConnector:318 - Stopped Spark@1e60c5cc{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2020-09-21 01:55:14 INFO  SparkUI:54 - Stopped Spark web UI at http://81c68c0b4a46:4040
2020-09-21 01:55:14 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!
2020-09-21 01:55:14 INFO  MemoryStore:54 - MemoryStore cleared
2020-09-21 01:55:14 INFO  BlockManager:54 - BlockManager stopped
2020-09-21 01:55:14 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped
2020-09-21 01:55:14 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!
2020-09-21 01:55:14 INFO  SparkContext:54 - Successfully stopped SparkContext
2020-09-21 01:55:14 INFO  ShutdownHookManager:54 - Shutdown hook called
2020-09-21 01:55:14 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-ac14f8c9-ec93-42a6-a743-34f5aa3dd7cb
2020-09-21 01:55:14 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-1bda6c2e-1fe0-44ac-bd25-3f95bbca0bcd
2020-09-21 01:55:14 INFO  ShutdownHookManager:54 - Deleting directory /tmp/temporaryReader-720178a4-c6b3-407b-831a-fd20b988c96f
2020-09-21 01:55:14 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-ac14f8c9-ec93-42a6-a743-34f5aa3dd7cb/pyspark-230da6b1-319d-4339-aab0-ea6c20980790
